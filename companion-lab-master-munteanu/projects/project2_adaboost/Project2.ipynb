{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Machine Learning, Fall 2022\n",
    "\n",
    "Deadline: 11th of December 2022, 23:59\n",
    "\n",
    "To do this project you have to complete this Jupyter notebook and send it via Discord.\n",
    "\n",
    "The total number of points allocated for this project is 10.\n",
    "\n",
    "You will need the following modules to solve the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sex  island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0    0       2            39.1           18.7              181.0       3750.0   \n",
      "1    1       2            39.5           17.4              186.0       3800.0   \n",
      "2    1       2            40.3           18.0              195.0       3250.0   \n",
      "3    1       2            36.7           19.3              193.0       3450.0   \n",
      "4    0       2            39.3           20.6              190.0       3650.0   \n",
      "\n",
      "   species  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "   sex  island  species\n",
      "0    0       2        1\n",
      "1    1       2        1\n",
      "2    1       2        1\n",
      "3    1       2        1\n",
      "4    0       2        1\n"
     ]
    }
   ],
   "source": [
    "penguin_dataset = pd.read_csv(\"data/penguins_filtered.csv\")\n",
    "\n",
    "penguin_dataset = penguin_dataset.replace({\n",
    "    \"Adelie\": 1,\n",
    "    \"Chinstrap\" : 2,\n",
    "    \"Gentoo\": 3,\n",
    "    \"male\" : 0,\n",
    "    \"female\" : 1,\n",
    "    \"Biscoe\" : 0,\n",
    "    \"Dream\" : 1,\n",
    "    \"Torgersen\" : 2})\n",
    "\n",
    "discrete_penguin_dataset = penguin_dataset[[\"sex\", \"island\", \"species\"]]\n",
    "print(penguin_dataset.head())\n",
    "print(discrete_penguin_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Naive and Joint Bayes (3.5 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: Be careful at what Naive Bayes class you use from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the prior probabilities for the target feature. Transform them by applying the natural logarithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the formulas used to calculate the maximum aposteriori probability using Naive Bayes and Joint Bayes. Use the names of the variables from the discrete penguin dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer here*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find and calculate the logarithm of all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why does the results contain infinity? Fix the calculation by using the Laplace Smoothing with `alpha = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the aposteriori probabilities of the labels and decide which label will Naive Bayes predict for the instance. Use only the logarithm values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Naive Bayes implemenation**: write a function called `naive_bayes` that takes three arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "- `alpha`: the parameter used for Laplace Smoothing\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `log_prior`: the logarithmic values of the prior probabilities (the probability of the labels)\n",
    "- `log_likelihoods`: a n x m x t array, where n - the number of features; m - the number of labels; t - the number of values for a feature; this array will contain the logarithmic values of the likelihoods (P(feature = value | target_feature = label))\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(df, index_target = -1, alpha = 1e-10):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the discrete penguin dataset using your version of Naive Bayes and sklearn's. Compare the values of the parameters.(Be careful at what type of Naive Bayes classifier you pick from sklearn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a function named `nb_predict_prob` that uses the log probabilities calculated by Naive Bayes to infer the aposteriori probability of a new instance `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predict_prob(nb_dict, X, use_log = False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a function that does the Naive Bayes prediction using the Maximum Aposteriori Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predict(nb_dict, X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a function that calculate the accuracy of the trained model on a set of instances `X` with known labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_score(nb_dict, X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Calculate the training accuracy of your Naive Bayes algorithm. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Find and calculate all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Joint Bayes. (*Hint*: panda's query function might provide itself useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Calculate the aposteriori probabilities of the labels and decide which label will Joint Bayes predict for the instance. Use the conditional and prior probabilities (*not* the logarithmic values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **Joint Bayes implemenation**: write a function called `joint_bayes` that takes two arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `prior_probs`: the prior probabilities (the probability of the labels)\n",
    "- `likelihoods`: a n x m array, where n - the number of labels; m - the number of combination between the values of the features; each label will have assigned a list containing the joint probability P(feature_1 = value_1, feature_2 = value_2, ...,  feature_n = value_n | target_feature = label)\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature).\n",
    "\n",
    "*Hint*: check the imports from the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_bayes(df, index_target = -1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Train the Joint Bayes algorithm on the discrete penguin dataset. Print the obtained dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Similarly to Naive Bayes, write the functions used to predict the aposteriori probabilities, the label and the accuracy of the Joint Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb_predict_prob(jb_dict, X):\n",
    "    pass\n",
    "\n",
    "def jb_predict(jb_dict, X):\n",
    "    pass\n",
    "\n",
    "def jb_score(jb_dict, X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "17. Calculate the training accuracy of your Joint Bayes algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. kNN (2 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will use the entire `penguin_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the Euclidean distance between the test instance `{\"sex\" : 1, \"island\" : 2, \"bill_length\" : 20, \"bill_depth\" : 40, \"flipper_length\" : 355, \"body_mass\" : 855}` and the instances from the dataset. Store the values in an object called `instance_distance`. Print the average distance. (*Hint*: check the norm function from the numpy package.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the `5` nearest neighbours of the test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine the probabilities of the labels that kNN would assign for this test instance (`k = 5`). Which label has the highest probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suppose that kNN gives for each neighbour a weight that is equal to the inverse of the distance. Print the changed probabilities, as well as the label predicted by kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **k-NN implementation**: Create a function `knn_predict_prob` that will take six arguments:\n",
    "- `df`: the dataframe containing the features and the target feature\n",
    "- `test_x`: a list with the attributes observed for *one* instance\n",
    "- `k`: the number of nearest neighbours\n",
    "- `use_weights`: boolean value that indicates whether to assign weights based on the inverse of the distance or not\n",
    "- `p`: either an integer, indicating the order of the Minkowski distance (p=2 is the equivalent for Euclidean) or a custom distance function\n",
    "- `index_target`: the index of the column that contains the labels of the target feature\n",
    "\n",
    "The function should:\n",
    "- calculate the distance between `test_x` and the observations from the dataset\n",
    "- extract the k-nearest neighbours\n",
    "- calculate the weight for each label\n",
    "- normalize the weights to become probabilities\n",
    "- return the probability vector, that will indicate the probability of the instance `test_x` to have the label `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict_prob(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "   pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function that, based on the probabilities calculated above, returns the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculate the probabilities and the predicted label for the instance from exercise 1 using the following configurations:\n",
    "- `k = 11, unweighted, Euclidean distance`\n",
    "- `k = 11, weighted, Euclidean distance`\n",
    "- `k = 11, unweighted, Manhattan distance`\n",
    "- `k = 11, weighted, Manhattan distance`\n",
    "\n",
    "Compare your results with the results obtained by the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function that calculates the accuracy of the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_score(df, test_x, test_y, k, use_weights = True, p = 2, index_target = -1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the training accuracy of the unweighted kNN when k varies from 3 to 15? (use only odd numbers). Does adding the weight / changing the distance metric improve the score? Justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. AdaBoost (4.5 points; 0.5 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes*: \n",
    "- The results might differ from the sklearn implementation\n",
    "- we will use the definition from the Machine Learning course of a weak classifier, which linearly separates an attribute\n",
    "- we try to use classifiers that minimise the error, not the entropy, as ID3\n",
    "- for getting the thresholds / splitting points, check the implementation from the Project 1 (mine or yours, whatever works for you)\n",
    "- remember the usage of the external threshold\n",
    "- some exercises from the course's book contain useful tricks to reduce the number of calculation you make during an iteration\n",
    "- the following implementation is a generalisation (with respect to the number of labels) of the AdaBoost algorithm that was discussed at the course\n",
    "    - suppose the target feature `Y` has `n` labels (i.e. `y1`, `y2` ... `yn`)\n",
    "    - suppose you have a feature `X` and a threshold / splitting point `x`\n",
    "    - on the right side of the separator (so for `X > x`), we have `k1` points with label `y1`, `k2` points with label `y2` and so on\n",
    "    - the label that the weak classifier will predict for the surface `X > x` will be the one with the highest probabilities of the points\n",
    "    - in other words, the label will be $\\displaystyle \\argmax_{y \\in Val(Y)} \\sum_{point \\in X, point > x; \\;\\; label(point) = y} D_t(point)$\n",
    "    - we perform similar calculation for the left side of the separator\n",
    "    - in the end, we will get the two labels that the weak classifier will assign\n",
    "    - the error of the classifier will be the sum of the probabilities of the points that are misclasified\n",
    "- please truncate your results; use 7 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a numpy array named `D1` that contains the initial probability distribution of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = np.array([])\n",
    "# print(D1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the number of the available weak estimators used in our version of AdaBoost? (include the external threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For the attribute `flipper_length_mm` and the threshold 206.5, find the sum of probabilities of the three species for both left and right side of the separator. (for example, on the right side we will have probability `a` for `species=1`, `b` for `species=2`, `c` for `species=3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the label that the weak classifier will assign for the right side of the separator? But for the left side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the error of this weak classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the label the weak classifier associated with the external split point will predict? What about its error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a function that calculates the error of a weak classifier. Test your function on the classifier mentioned in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_error_classifier(df, feature_index, threshold, target_feature, probs):\n",
    "    pass\n",
    "\n",
    "print(get_error_classifier(penguin_dataset, 4, 206.5, \"species\", D1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function that identifies the weak estimator with the lowest error. Do not forget to include the external split point. What is the best weak estimator that will be chosen at the first iteration of AdaBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "def get_best_weak_estimator(df, target_feature, current_probs):\n",
    "    # should return a tuple with two elements: the estimator (also a tuple with the index of the feature, the threshold, the label assigned in the left side, the label from the right side) and its error\n",
    "    return (None, None)\n",
    "\n",
    "est1, eps1 = get_best_weak_estimator(penguin_dataset, \"species\", D1)\n",
    "print(est1, eps1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Calculate the weight `alpha_1` that AdaBoost will assign to this weak classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Update the probability distribution `D2` that will be used in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andi/.local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/andi/.local/lib/python3.11/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "D2 = []\n",
    "\n",
    "print(sum(D2))\n",
    "print(np.mean(D2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Write a function that performs the update of the probability distribution. Test the function for the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def update_probs(df, estimator, target_feature, eps, current_probs):\n",
    "    return np.array([])\n",
    "\n",
    "print(np.sum(update_probs(penguin_dataset, est1, \"species\", eps1, D1) - D2 < 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Write a function that trains the AdaBoost algorithm on a dataset. The function should return five fields:\n",
    "- `estimators`: a list of the weak estimators\n",
    "- `estimators_error`: a list with the errors of the estimators\n",
    "- `estimators_weight`: a list with the weights assigned to each estimator\n",
    "- `n_iters`: the number of iterations\n",
    "- `features`: a list with the features used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def adaboost(df, target_feature, n_iters):\n",
    "    pass\n",
    "\n",
    "my_ab = adaboost(penguin_dataset, \"species\", 5)\n",
    "print(my_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Write a function that does the AdaBoost prediction of an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adaboost_predict(adaboost_model, X):\n",
    "    pass\n",
    "\n",
    "adaboost_predict(my_ab, penguin_dataset[penguin_dataset.columns[:-1]].iloc[0]), penguin_dataset[\"species\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write a function that calculates the accuracy of AdaBoost on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def adaboost_score(adaboost_model, X, y):\n",
    "    pass\n",
    "\n",
    "print(adaboost_score(my_ab, penguin_dataset[penguin_dataset.columns[:-1]].to_numpy(), penguin_dataset[\"species\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Draw a plot where you compare the evolution of accuracy of our implementation of AdaBoost and sklearn's. The number of iterations should vary between 1 and 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise grading\n",
    "\n",
    "| Section | Exercise | Points |\n",
    "| --- | --- | --- |\n",
    "| I |\t1 | \t0.1 |\n",
    "| I |\t2 | \t0.1 |\n",
    "| I |\t3 | \t0.25 |\n",
    "| I |\t4 | \t0.25 |\n",
    "| I |\t5 | \t0.15 |\n",
    "| I |\t6 | \t0.65 |\n",
    "| I |\t7 | \t0.1 |\n",
    "| I |\t8 | \t0.25 |\n",
    "| I |\t9 | \t0.15 |\n",
    "| I |\t10 | \t0.1 |\n",
    "| I |\t11 | \t0.1 |\n",
    "| I |\t12 | \t0.1 |\n",
    "| I |\t13 | \t0.15 |\n",
    "| I |\t14 | \t0.65 |\n",
    "| I |\t15 | \t0.1 |\n",
    "| I |\t16 | \t0.2 |\n",
    "| I |\t17 | \t0.1 |\n",
    "|II | \t1 | \t0.2 |\n",
    "|II | \t2 | \t0.1 |\n",
    "|II | \t3 | \t0.15 |\n",
    "|II | \t4 | \t0.2 |\n",
    "|II | \t5 | \t0.72 |\n",
    "|II | \t6 | \t0.1 |\n",
    "|II | \t7 | \t0.28 |\n",
    "|II | \t8 | \t0.1 |\n",
    "|II | \t9 | \t0.15 |\n",
    "|III |  1 | 0.1 |\n",
    "|III |  2 | 0.2 |\n",
    "|III |  3 | 0.3 |\n",
    "|III |  4 | 0.15 |\n",
    "|III |  5 | 0.15 |\n",
    "|III |  6 | 0.3 |\n",
    "|III |  7 | 0.5 |\n",
    "|III |  8 | 0.4 |\n",
    "|III |  9 | 0.1 |\n",
    "|III |  10 | 0.5 |\n",
    "|III |  11 | 0.2 |\n",
    "|III |  12 | 1.1 |\n",
    "|III |  13 | 0.2 |\n",
    "|III |  14 | 0.1 |\n",
    "|III |  15 | 0.2 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
