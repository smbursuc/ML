{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Machine Learning, Fall 2022\n",
    "\n",
    "Deadline: 11th of December 2022, 23:59\n",
    "\n",
    "To do this project you have to complete this Jupyter notebook and send it via Discord.\n",
    "\n",
    "The total number of points allocated for this project is 10.\n",
    "\n",
    "You will need the following modules to solve the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sex  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0      0       2            39.1           18.7              181.0   \n",
      "1      1       2            39.5           17.4              186.0   \n",
      "2      1       2            40.3           18.0              195.0   \n",
      "3      1       2            36.7           19.3              193.0   \n",
      "4      0       2            39.3           20.6              190.0   \n",
      "..   ...     ...             ...            ...                ...   \n",
      "328    0       1            55.8           19.8              207.0   \n",
      "329    1       1            43.5           18.1              202.0   \n",
      "330    0       1            49.6           18.2              193.0   \n",
      "331    0       1            50.8           19.0              210.0   \n",
      "332    1       1            50.2           18.7              198.0   \n",
      "\n",
      "     body_mass_g  species  \n",
      "0         3750.0        1  \n",
      "1         3800.0        1  \n",
      "2         3250.0        1  \n",
      "3         3450.0        1  \n",
      "4         3650.0        1  \n",
      "..           ...      ...  \n",
      "328       4000.0        2  \n",
      "329       3400.0        2  \n",
      "330       3775.0        2  \n",
      "331       4100.0        2  \n",
      "332       3775.0        2  \n",
      "\n",
      "[333 rows x 7 columns]\n",
      "   sex  island  species\n",
      "0    0       2        1\n",
      "1    1       2        1\n",
      "2    1       2        1\n",
      "3    1       2        1\n",
      "4    0       2        1\n"
     ]
    }
   ],
   "source": [
    "penguin_dataset = pd.read_csv(\"data/penguins_filtered.csv\")\n",
    "\n",
    "penguin_dataset = penguin_dataset.replace({\n",
    "    \"Adelie\": 1,\n",
    "    \"Chinstrap\" : 2,\n",
    "    \"Gentoo\": 3,\n",
    "    \"male\" : 0,\n",
    "    \"female\" : 1,\n",
    "    \"Biscoe\" : 0,\n",
    "    \"Dream\" : 1,\n",
    "    \"Torgersen\" : 2})\n",
    "\n",
    "discrete_penguin_dataset = penguin_dataset[[\"sex\", \"island\", \"species\"]]\n",
    "print(penguin_dataset)\n",
    "print(discrete_penguin_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Naive and Joint Bayes (3.5 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the prior probabilities for the target feature. Transform them by applying the natural logarithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43843843843843844, 0.2042042042042042, 0.35735735735735735]\n",
      "[-0.8245358682721073, -1.5886347848043372, -1.0290189968689145]\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "species, species_nr = np.unique(penguin_dataset[\"species\"], return_counts=True)\n",
    "p_species = []\n",
    "for specie_nr in species_nr:\n",
    "    p_species.append(specie_nr / sum(species_nr))\n",
    "\n",
    "print(p_species)\n",
    "\n",
    "for i in range(len(p_species)):\n",
    "    p_species[i] = np.log(p_species[i])\n",
    "\n",
    "print(p_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the formulas used to calculate the maximum aposteriori probability using Naive Bayes and Joint Bayes. Use the names of the variables from the discrete penguin dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer here*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAIVE BAYES: v_MAP = argmax P(a_i|v_j)P(v_j), where a_i is an attribute and v_j is target atrribute value\n",
    "#JOINT BAYES: v_MAP = argmax P(a_1,a_2,...,a_n|v_j)P(v_j)\n",
    "\n",
    "#naive bayes\n",
    "#P(sex=x|species=1)P(island=y|species=1)P(species=1), x in {0,1}, y in {0,1,2}\n",
    "\n",
    "#joint bayes\n",
    "#P(sex=x,island=y|species=1)P(species=1), x in {0,1}, y in {0,1,2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find and calculate the logarithm of all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sex=1|species=1) = 0.5\n",
      "ln(P(sex=1|species=1) = 0.5) = -0.6931471805599453\n",
      "\n",
      "P(sex=1|species=2) = 0.5\n",
      "ln(P(sex=1|species=2) = 0.5) = -0.6931471805599453\n",
      "\n",
      "P(sex=1|species=3) = 0.48739495798319327\n",
      "ln(P(sex=1|species=3) = 0.48739495798319327) = -0.7186804825651101\n",
      "\n",
      "P(island=2|species=1) = 0.3219178082191781\n",
      "ln(P(island=2|species=1) = 0.3219178082191781) = -1.133459019998278\n",
      "\n",
      "P(island=2|species=2) = 0\n",
      "ln(P(island=2|species=2) = 0) = -inf\n",
      "\n",
      "P(island=2|species=3) = 0\n",
      "ln(P(island=2|species=3) = 0) = -inf\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godar\\AppData\\Local\\Temp\\ipykernel_5528\\2751706160.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  print(\"ln(\" + res_string + \") = \" + str(np.log(p_tf_val)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.3219178082191781, 2: 0, 3: 0}"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "#P(sex=1|species=1),P(island=2|species=1)\n",
    "#P(sex=1|species=2),P(island=2|species=2)\n",
    "#P(sex=1|species=3),P(island=2|species=3)\n",
    "\n",
    "def calculate_conditional_probability(dataset, feature, target_feature, alpha, print_res = True):\n",
    "    feature_vals, nr_feature_vals = np.unique(dataset[feature], return_counts=True)\n",
    "    tf_name = list(target_feature.keys())[0]\n",
    "    tf_val = target_feature[tf_name]\n",
    "    dict = {}\n",
    "    for feature_val in feature_vals:\n",
    "        temp_dataset = dataset[dataset[feature] == feature_val]\n",
    "        target_feature_vals, nr_target_feature_vals = np.unique(temp_dataset[tf_name], return_counts=True)\n",
    "        #print(target_feature_vals, nr_target_feature_vals)\n",
    "        i_tf_val = -1\n",
    "        #print(target_feature_vals, nr_target_feature_vals)\n",
    "        for i in range(len(target_feature_vals)):\n",
    "            if target_feature_vals[i] == tf_val:\n",
    "                i_tf_val = i\n",
    "                break\n",
    "        #print(i_tf_val, tf_val)\n",
    "        #print(feature_val, nr_target_feature_vals[i_tf_val])\n",
    "        if alpha==0:\n",
    "            if i_tf_val == -1: #if target feature is not found\n",
    "                p_tf_val = 0\n",
    "            else:\n",
    "                p_tf_val = nr_target_feature_vals[i_tf_val] / sum(nr_target_feature_vals)\n",
    "        else:\n",
    "            laplace_denom = len(np.unique(penguin_dataset[tf_name]))\n",
    "            if i_tf_val == -1: #if target feature is not found\n",
    "                p_tf_val = 1 / (sum(nr_target_feature_vals) + laplace_denom)\n",
    "            else:\n",
    "                p_tf_val = (nr_target_feature_vals[i_tf_val] + alpha) / (sum(nr_target_feature_vals) + laplace_denom)\n",
    "            #print(nr_target_feature_vals[i_tf_val] + alpha, sum(nr_target_feature_vals) + laplace_denom)\n",
    "        res_string = \"P(\" + tf_name + \"=\" + str(tf_val) + \"|\" + feature + \"=\" + str(feature_val) + \") = \" + str(p_tf_val)\n",
    "        if print_res:\n",
    "            print(res_string)\n",
    "            print(\"ln(\" + res_string + \") = \" + str(np.log(p_tf_val)))\n",
    "            print()\n",
    "        dict[feature_val] = p_tf_val\n",
    "    return dict\n",
    "\n",
    "\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"sex\" : 1},0)\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"island\" : 2},0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why does the results contain infinity? Fix the calculation by using the Laplace Smoothing with `alpha = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sex=1|species=1) = 0.5\n",
      "ln(P(sex=1|species=1) = 0.5) = -0.6931471805599453\n",
      "\n",
      "P(sex=1|species=2) = 0.5\n",
      "ln(P(sex=1|species=2) = 0.5) = -0.6931471805599453\n",
      "\n",
      "P(sex=1|species=3) = 0.48760330578512395\n",
      "ln(P(sex=1|species=3) = 0.48760330578512395) = -0.7182531016910216\n",
      "\n",
      "P(island=2|species=1) = 0.3221476510067114\n",
      "ln(P(island=2|species=1) = 0.3221476510067114) = -1.1327452950375683\n",
      "\n",
      "P(island=2|species=2) = 0.014084507042253521\n",
      "ln(P(island=2|species=2) = 0.014084507042253521) = -4.2626798770413155\n",
      "\n",
      "P(island=2|species=3) = 0.00819672131147541\n",
      "ln(P(island=2|species=3) = 0.00819672131147541) = -4.804021044733257\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.3221476510067114, 2: 0.014084507042253521, 3: 0.00819672131147541}"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "# the results contain infinity because the posterior probability of some values is 0 and ln(0) is -infinity\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"sex\" : 1},1)\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"island\" : 2},1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the aposteriori probabilities of the labels and decide which label will Naive Bayes predict for the instance. Use only the logarithm values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sex=1|species=1) = 0.5\n",
      "ln(P(sex=1|species=1) = 0.5) = -0.6931471805599453\n",
      "\n",
      "P(sex=1|species=2) = 0.5\n",
      "ln(P(sex=1|species=2) = 0.5) = -0.6931471805599453\n",
      "\n",
      "P(sex=1|species=3) = 0.48760330578512395\n",
      "ln(P(sex=1|species=3) = 0.48760330578512395) = -0.7182531016910216\n",
      "\n",
      "P(island=2|species=1) = 0.3221476510067114\n",
      "ln(P(island=2|species=1) = 0.3221476510067114) = -1.1327452950375683\n",
      "\n",
      "P(island=2|species=2) = 0.014084507042253521\n",
      "ln(P(island=2|species=2) = 0.014084507042253521) = -4.2626798770413155\n",
      "\n",
      "P(island=2|species=3) = 0.00819672131147541\n",
      "ln(P(island=2|species=3) = 0.00819672131147541) = -4.804021044733257\n",
      "\n",
      "-2.6504283438696206\n",
      "-6.544461842405598\n",
      "-6.551293143293193\n",
      "\n",
      "MAP label: 1\n",
      "[array([[-0.69314718, -0.69314718],\n",
      "       [-0.69314718, -0.69314718],\n",
      "       [-0.66865616, -0.7182531 ]]), array([[-1.19728382, -0.97859462, -1.1327453 ],\n",
      "       [-4.26267988, -0.02857337, -4.26267988],\n",
      "       [-0.0165293 , -4.80402104, -4.80402104]])]\n",
      "[1]\n",
      "[[0.96099563 0.0195688  0.01943557]]\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "#P(sex=1|species=1)P(island=2|species=1)P(species=1)\n",
    "#P(sex=1|species=2)P(island=2|species=2)P(species=2)\n",
    "#P(sex=1|species=3)P(island=2|species=3)P(species=3)\n",
    "\n",
    "p_sex = calculate_conditional_probability(penguin_dataset,\"species\",{ \"sex\" : 1},1)\n",
    "p_island = calculate_conditional_probability(penguin_dataset,\"species\",{ \"island\" : 2},1)\n",
    "# print(p_sex)\n",
    "# print(p_island)\n",
    "\n",
    "feature_vals, nr_feature_vals = np.unique(penguin_dataset[\"species\"], return_counts=True)\n",
    "p_species = []\n",
    "for specie_nr in nr_feature_vals:\n",
    "    p_species.append(specie_nr / sum(nr_feature_vals))\n",
    "\n",
    "# print(nr_feature_vals)\n",
    "# print(p_species)\n",
    "max_label = -1\n",
    "max_prob = -9999999999\n",
    "for val in feature_vals:\n",
    "    prob = np.log(p_sex[val]) + np.log(p_island[val]) + np.log(p_species[val-1])\n",
    "    print(prob)\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max_label = val\n",
    "\n",
    "print()\n",
    "print(\"MAP label: \" + str(max_label))\n",
    "\n",
    "\n",
    "#VERIFICARE\n",
    "#2 ore sa stau sa realizez ca imi trebuie categorical bayes si nu bernoulli bayes asdasdapsdeoqansoieqpwoe1po23op1231\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "X = penguin_dataset[[\"sex\", \"island\"]]\n",
    "y = penguin_dataset[\"species\"]\n",
    "cl = CategoricalNB(alpha=1).fit(X, y)\n",
    "\n",
    "new_instance = pd.DataFrame(\n",
    "  [(1, 2)],\n",
    "columns = [\"sex\",\"island\"])\n",
    "\n",
    "print(cl.feature_log_prob_)\n",
    "print(cl.predict(new_instance))\n",
    "print(cl.predict_proba(new_instance))\n",
    "\n",
    "#print(np.unique(penguin_dataset[penguin_dataset[\"species\"] == 3].filter(items=[\"island\"]),return_counts=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Naive Bayes implemenation**: write a function called `naive_bayes` that takes three arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "- `alpha`: the parameter used for Laplace Smoothing\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `log_prior`: the logarithmic values of the prior probabilities (the probability of the labels)\n",
    "- `log_likelihoods`: a n x m x t array, where n - the number of features; m - the number of labels; t - the number of values for a feature; this array will contain the logarithmic values of the likelihoods (P(feature = value | target_feature = label))\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{0: {1: 0.5, 2: 0.5, 3: 0.512396694214876}, 1: {1: 0.5, 2: 0.5, 3: 0.48760330578512395}}], [{0: {1: 0.30201342281879195, 2: 0.014084507042253521, 3: 0.9836065573770492}, 1: {1: 0.37583892617449666, 2: 0.971830985915493, 3: 0.00819672131147541}, 2: {1: 0.3221476510067114, 2: 0.014084507042253521, 3: 0.00819672131147541}}]]\n",
      "\n",
      "Accuracy: 0.7027027027027027\n",
      "Accuracy sklearn: 0.7027027027027027\n",
      "\n",
      "{'n_classes': 3, 'classes': [1, 2, 3], 'log_prior': {1: -0.8245358682721073, 2: -1.5886347848043372, 3: -1.0290189968689145}, 'n_features_classes': [[0, 1], [0, 1, 2]], 'log_likelihood': [[[-0.6931471805599453, -0.6931471805599453], [-0.6931471805599453, -0.6931471805599453], [-0.6686561605516496, -0.7182531016910216]], [[-1.1972838161751393, -0.9785946152103099, -1.1327452950375683], [-4.2626798770413155, -0.028573372444056, -4.2626798770413155], [-0.016529301951210582, -4.804021044733257, -4.804021044733257]]]}\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(df, index_target = -1, alpha = 1e-10, display_vals = False):\n",
    "    result_dictionary = {}\n",
    "    target_feature = df.columns[index_target]\n",
    "    features = []\n",
    "    for column in df.columns:\n",
    "        if column != target_feature:\n",
    "            features.append(column)\n",
    "    tf_vals, nr_tf_vals = np.unique(df[target_feature], return_counts=True)\n",
    "    result_dictionary[\"n_classes\"] = len(tf_vals)\n",
    "    result_dictionary[\"classes\"] = list(tf_vals)\n",
    "    p_tf = []\n",
    "    for tf_val in nr_tf_vals:\n",
    "        p_tf.append(tf_val / sum(nr_tf_vals))\n",
    "    p_tf_ln = []\n",
    "    for i in range(len(p_tf)):\n",
    "        p_tf_ln.append(np.log(p_tf[i]))\n",
    "    dict = {}\n",
    "    for i in range(len(tf_vals)):\n",
    "        dict[tf_vals[i]] = p_tf_ln[i]\n",
    "    result_dictionary[\"log_prior\"] = dict\n",
    "    wrong = 0\n",
    "    log_likelihoods = []\n",
    "    p_fs = []\n",
    "    n_feature_classes = []\n",
    "    for feature in features:\n",
    "        f_vals, nr_f_vals = np.unique(df[feature], return_counts=True)\n",
    "        vect = []\n",
    "        for f_val in f_vals:\n",
    "            vect.append(f_val)\n",
    "        n_feature_classes.append(vect)\n",
    "    result_dictionary[\"n_features_classes\"] = n_feature_classes\n",
    "    #calculate conditional entropies returneaza un dictionar de format {valoare_target_feature1 : probabilitate1, valoare_target_feature2 : probabilitate2, ...} pentru o valoare feature\n",
    "    #aici asociez valoarea in sine a unui feature dictionarul pomenit mai sus si dupa creez o matrice\n",
    "    #am facut asta stiind ca un vector din matrice ii corespunde unui feature\n",
    "    for feature in features:\n",
    "        f_vals, nr_f_vals = np.unique(df[feature], return_counts=True)\n",
    "        p_f = {}\n",
    "        vect = []\n",
    "        for f_val in f_vals:\n",
    "            p_f[f_val] = calculate_conditional_probability(df, target_feature, {feature : f_val}, alpha, False)\n",
    "        vect.append(p_f)\n",
    "        p_fs.append(vect)\n",
    "    if display_vals:\n",
    "        print(p_fs)\n",
    "        print()\n",
    "    # formatare cum se cere in enunt sau mai bine zis cum afiseaza categoricalNB pentru atributul feature_log_prob_\n",
    "    for p_f in p_fs:\n",
    "        vect2 = []\n",
    "        for tf_val in tf_vals:\n",
    "            vect = []\n",
    "            for key,value in p_f[0].items():\n",
    "                vect.append(np.log(p_f[0][key][tf_val]))\n",
    "            vect2.append(vect)\n",
    "        log_likelihoods.append(vect2)\n",
    "    #nu e necesar dar am vrut sa ma verific imediat sa vad daca am muncit degeaba LOL XD :DDDD\n",
    "    for index, row in df.iterrows():\n",
    "        p_max = -99999999\n",
    "        max_tf_val = -1\n",
    "        all_cond_p = []\n",
    "        for feature in features:\n",
    "            all_cond_p.append(calculate_conditional_probability(df, target_feature, {feature: df.at[index, feature]}, alpha, False))\n",
    "        p = 1\n",
    "        for tf_val in tf_vals:\n",
    "            p = 1\n",
    "            for cond_p in all_cond_p:\n",
    "                p *= cond_p[tf_val]\n",
    "            p *= p_tf[tf_val-1]\n",
    "            if p > p_max:\n",
    "                p_max = p\n",
    "                max_tf_val = tf_val\n",
    "        if max_tf_val != df.at[index, target_feature]:\n",
    "            wrong += 1\n",
    "    result_dictionary[\"log_likelihood\"] = log_likelihoods\n",
    "    if display_vals:\n",
    "        print(\"Accuracy: \" + str((len(df.index)-wrong) / len(df.index)))\n",
    "        print(\"Accuracy sklearn:\", cl.score(df[features], df[target_feature]))\n",
    "        print()\n",
    "    return(result_dictionary)\n",
    "\n",
    "print(naive_bayes(discrete_penguin_dataset, 2, 1, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the discrete penguin dataset using your version of Naive Bayes and sklearn's. Compare the values of the parameters.(Be careful at what type of Naive Bayes classifier you pick from sklearn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7027027027027027\n",
      "Accuracy sklearn: 0.7027027027027027\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "result_dictionary = naive_bayes(discrete_penguin_dataset, 2, 1)\n",
    "df = discrete_penguin_dataset\n",
    "target_feature = \"species\"\n",
    "alpha = 0\n",
    "features = []\n",
    "for column in df.columns:\n",
    "    if column != target_feature:\n",
    "        features.append(column)\n",
    "wrong = 0\n",
    "for index, row in df.iterrows():\n",
    "        p_max = -99999999\n",
    "        max_tf_val = -1\n",
    "        all_cond_p = []\n",
    "        for class_ in result_dictionary[\"classes\"]:\n",
    "            p = 0\n",
    "            for i in range(len(result_dictionary[\"n_features_classes\"])):\n",
    "                p += result_dictionary[\"log_likelihood\"][i][class_-1][df.at[index, features[i]]]\n",
    "            p += result_dictionary[\"log_prior\"][class_]\n",
    "            if p > p_max:\n",
    "                p_max = p\n",
    "                max_tf_val = class_\n",
    "        if max_tf_val != df.at[index, target_feature]:\n",
    "            wrong += 1\n",
    "print(\"Accuracy: \" + str((len(df.index)-wrong) / len(df.index)))\n",
    "print(\"Accuracy sklearn:\", cl.score(df[features], df[target_feature]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a function named `nb_predict_prob` that uses the log probabilities calculated by Naive Bayes to infer the aposteriori probability of a new instance `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_classes': 3, 'classes': [1, 2, 3], 'log_prior': {1: -0.8245358682721073, 2: -1.5886347848043372, 3: -1.0290189968689145}, 'n_features_classes': [[0, 1], [0, 1, 2]], 'log_likelihood': [[[-0.6931471805599453, -0.6931471805599453], [-0.6931471805599453, -0.6931471805599453], [-0.6686561605516496, -0.7182531016910216]], [[-1.1972838161751393, -0.9785946152103099, -1.1327452950375683], [-4.2626798770413155, -0.028573372444056, -4.2626798770413155], [-0.016529301951210582, -4.804021044733257, -4.804021044733257]]]}\n",
      "\n",
      "[0.960995628673782, 0.019568798053176174, 0.01943557327304176]\n"
     ]
    }
   ],
   "source": [
    "print(result_dictionary)\n",
    "def nb_predict_prob(nb_dict, X, use_log = False):\n",
    "    result_dictionary = nb_dict\n",
    "    all_p = []\n",
    "    for class_ in result_dictionary[\"classes\"]:\n",
    "        p = 1\n",
    "        for i in range(len(result_dictionary[\"n_features_classes\"])):\n",
    "            p += result_dictionary[\"log_likelihood\"][i][class_-1][int(X.iloc[[0]].iloc[:, i])]\n",
    "        p += result_dictionary[\"log_prior\"][class_]\n",
    "        all_p.append(math.pow(math.e, p)) # formula folosita pt a obtine pondere mai mare pt valoare mai mica\n",
    "    predict_proba_total = sum(all_p)\n",
    "    predict_proba = []\n",
    "    for p in all_p:\n",
    "        predict_proba.append(p / predict_proba_total)\n",
    "    return predict_proba\n",
    "\n",
    "new_instance = pd.DataFrame(\n",
    "  [(1, 2)],\n",
    "columns = [\"sex\",\"island\"])\n",
    "\n",
    "print()\n",
    "print(nb_predict_prob(result_dictionary, new_instance))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a function that does the Naive Bayes prediction using the Maximum Aposteriori Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def nb_predict(nb_dict, X):\n",
    "    result_dictionary = nb_dict\n",
    "    p_max = -99999999\n",
    "    max_tf_val = -1\n",
    "    all_cond_p = []\n",
    "    for class_ in result_dictionary[\"classes\"]:\n",
    "        p = 0\n",
    "        for i in range(len(result_dictionary[\"n_features_classes\"])):\n",
    "            p += result_dictionary[\"log_likelihood\"][i][class_-1][int(X.iloc[[0]].iloc[:, i])]\n",
    "        p += result_dictionary[\"log_prior\"][class_]\n",
    "        if p > p_max:\n",
    "            p_max = p\n",
    "            max_tf_val = class_\n",
    "    return max_tf_val\n",
    "\n",
    "new_instance = pd.DataFrame(\n",
    "  [(1, 2)],\n",
    "columns = [\"sex\",\"island\"])\n",
    "print(nb_predict(result_dictionary, new_instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a function that calculate the accuracy of the trained model on a set of instances `X` with known labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_score(nb_dict, X, y):\n",
    "    wrong = 0\n",
    "    for i in range(len(X.index)):\n",
    "        y_label = int(y.iloc[[i]])\n",
    "        y_pred = nb_predict(nb_dict, X.iloc[[i]])\n",
    "        if y_label != y_pred:\n",
    "            wrong += 1\n",
    "    return (len(X.index)-wrong) / len(X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Calculate the training accuracy of your Naive Bayes algorithm. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7027027027027027\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "# Exista date inconsistente in tabel deci acuratatea nu poate fi 100%\n",
    "print(nb_score(result_dictionary, discrete_penguin_dataset[features], discrete_penguin_dataset[target_feature]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Find and calculate all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Joint Bayes. (*Hint*: panda's query function might provide itself useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex==1 and island==2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.1643835616438356, 2: 0.0, 3: 0.0}"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "#P(sex=1,island=2|species=1)\n",
    "#P(sex=1,island=2|species=2)\n",
    "#P(sex=1,island=2|species=3)\n",
    "\n",
    "def calculate_conditional_probability_JB(dataset, feature, target_feature, print_res = True):\n",
    "    feature_vals, nr_feature_vals = np.unique(dataset[feature], return_counts=True)\n",
    "    p_f_vals = []\n",
    "    for i in range(len(feature_vals)):\n",
    "        p_f_vals.append(nr_feature_vals[i] / sum(nr_feature_vals))\n",
    "    query = \"\"\n",
    "    for key, value in target_feature.items():\n",
    "        query += key + \"==\" + str(value) + \" and \"\n",
    "    query = query[:-5]\n",
    "    print(query)\n",
    "    dict = {}\n",
    "    for feature_val in feature_vals:\n",
    "        temp_dataset1 = dataset[dataset[feature] == feature_val]\n",
    "        temp_dataset2 = temp_dataset1.query(query)\n",
    "        nr_rows = len(temp_dataset2.index)\n",
    "        dict[feature_val] = nr_rows / len(temp_dataset1.index)\n",
    "    return dict\n",
    "        \n",
    "\n",
    "\n",
    "calculate_conditional_probability_JB(discrete_penguin_dataset,\"species\",{ \"sex\" : 1, \"island\" : 2})\n",
    "# df = pd.DataFrame({'A': range(1, 6),\n",
    "#                    'B': range(10, 0, -2),\n",
    "#                    'C C': range(10, 5, -1)})\n",
    "# print(df)\n",
    "# print(df.query('A < B and A==1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Calculate the aposteriori probabilities of the labels and decide which label will Joint Bayes predict for the instance. Use the conditional and prior probabilities (*not* the logarithmic values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex==1 and island==2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "p_instance = calculate_conditional_probability_JB(discrete_penguin_dataset,\"species\",{ \"sex\" : 1, \"island\" : 2})\n",
    "p_max = -999999999\n",
    "max_label = -1\n",
    "feature_vals, nr_feature_vals = np.unique(discrete_penguin_dataset[\"species\"], return_counts=True)\n",
    "p_f_vals = []\n",
    "for i in range(len(feature_vals)):\n",
    "    p_f_vals.append(nr_feature_vals[i] / sum(nr_feature_vals))\n",
    "for key,value in p_instance.items():\n",
    "    p = value * p_f_vals[key-1]\n",
    "    if p > p_max:\n",
    "        p_max = p\n",
    "        max_label = key\n",
    "print(max_label)\n",
    "\n",
    "#verificare cu librarie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **Joint Bayes implemenation**: write a function called `joint_bayes` that takes two arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `prior_probs`: the prior probabilities (the probability of the labels)\n",
    "- `likelihoods`: a n x m array, where n - the number of labels; m - the number of combination between the values of the features; each label will have assigned a list containing the joint probability P(feature_1 = value_1, feature_2 = value_2, ...,  feature_n = value_n | target_feature = label)\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature).\n",
    "\n",
    "*Hint*: check the imports from the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex==0 and island==0\n",
      "sex==0 and island==1\n",
      "sex==0 and island==2\n",
      "sex==1 and island==0\n",
      "sex==1 and island==1\n",
      "sex==1 and island==2\n",
      "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
      "\n",
      "[{'sex': 0, 'island': 0}, {'sex': 0, 'island': 1}, {'sex': 0, 'island': 2}, {'sex': 1, 'island': 0}, {'sex': 1, 'island': 1}, {'sex': 1, 'island': 2}]\n",
      "\n",
      "[{1: 0.1506849315068493, 2: 0.0, 3: 0.5126050420168067}, {1: 0.1917808219178082, 2: 0.5, 3: 0.0}, {1: 0.15753424657534246, 2: 0.0, 3: 0.0}, {1: 0.1506849315068493, 2: 0.0, 3: 0.48739495798319327}, {1: 0.18493150684931506, 2: 0.5, 3: 0.0}, {1: 0.1643835616438356, 2: 0.0, 3: 0.0}]\n",
      "\n",
      "[[0.1506849315068493, 0.1917808219178082, 0.15753424657534246, 0.1506849315068493, 0.18493150684931506, 0.1643835616438356], [0.0, 0.5, 0.0, 0.0, 0.5, 0.0], [0.5126050420168067, 0.0, 0.0, 0.48739495798319327, 0.0, 0.0]]\n",
      "{'prior_probs': [0.43843843843843844, 0.2042042042042042, 0.35735735735735735], 'classes': [1, 2, 3], 'n_features_classes': [[0, 1], [0, 1, 2]], 'n_classes': 3, 'likelihoods': [[0.1506849315068493, 0.1917808219178082, 0.15753424657534246, 0.1506849315068493, 0.18493150684931506, 0.1643835616438356], [0.0, 0.5, 0.0, 0.0, 0.5, 0.0], [0.5126050420168067, 0.0, 0.0, 0.48739495798319327, 0.0, 0.0]]}\n"
     ]
    }
   ],
   "source": [
    "def joint_bayes(df, index_target = -1, display = False):\n",
    "    result_dictionary = {}\n",
    "    target_feature = df.columns[index_target]\n",
    "    feature_vals, nr_feature_vals = np.unique(df[target_feature], return_counts=True)\n",
    "    p_f_vals = []\n",
    "    for i in range(len(feature_vals)):\n",
    "        p_f_vals.append(nr_feature_vals[i] / sum(nr_feature_vals))\n",
    "    result_dictionary[\"prior_probs\"] = p_f_vals\n",
    "    result_dictionary[\"classes\"] = list(np.unique(df[target_feature]))\n",
    "    features = []\n",
    "    for column in df.columns:\n",
    "        if column != target_feature:\n",
    "            features.append(column)\n",
    "    n_feature_classes = []\n",
    "    for feature in features:\n",
    "        n_feature_classes.append(list(np.unique(df[feature])))\n",
    "    result_dictionary[\"n_features_classes\"] = n_feature_classes\n",
    "    result_dictionary[\"n_classes\"] = len(np.unique(df[target_feature]))\n",
    "\n",
    "    params = []\n",
    "    for feature in features:\n",
    "        params.append(np.unique(df[feature]))\n",
    "    cartesian_product = list(product(*params))\n",
    "\n",
    "    cond_prob_jb_params = []\n",
    "    for i in range(len(cartesian_product)):\n",
    "        cond_prob_jb_param = {}\n",
    "        for j in range(len(features)):\n",
    "            cond_prob_jb_param[features[j]] = cartesian_product[i][j]\n",
    "        cond_prob_jb_params.append(cond_prob_jb_param)\n",
    "    \n",
    "    all_p = []\n",
    "    for cond_prob_jb_param in cond_prob_jb_params:\n",
    "        p = calculate_conditional_probability_JB(df, target_feature, cond_prob_jb_param, print_res = False)\n",
    "        all_p.append(p)\n",
    "    \n",
    "    # transformare in formatul in care se cere in enunt :///// \n",
    "    likelihoods = []\n",
    "    for feature_val in feature_vals:\n",
    "        likelihood = []\n",
    "        for p in all_p:\n",
    "            likelihood.append(p[feature_val])\n",
    "        likelihoods.append(likelihood)\n",
    "\n",
    "\n",
    "    result_dictionary[\"likelihoods\"] = likelihoods\n",
    "    if display:\n",
    "        print(cartesian_product)\n",
    "        print()\n",
    "        print(cond_prob_jb_params)\n",
    "        print()\n",
    "        print(all_p)\n",
    "        print()\n",
    "        print(likelihoods)\n",
    "    return result_dictionary\n",
    "\n",
    "print(joint_bayes(discrete_penguin_dataset, 2, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Train the Joint Bayes algorithm on the discrete penguin dataset. Print the obtained dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex==0 and island==0\n",
      "sex==0 and island==1\n",
      "sex==0 and island==2\n",
      "sex==1 and island==0\n",
      "sex==1 and island==1\n",
      "sex==1 and island==2\n",
      "{'prior_probs': [0.43843843843843844, 0.2042042042042042, 0.35735735735735735], 'classes': [1, 2, 3], 'n_features_classes': [[0, 1], [0, 1, 2]], 'n_classes': 3, 'likelihoods': [[0.1506849315068493, 0.1917808219178082, 0.15753424657534246, 0.1506849315068493, 0.18493150684931506, 0.1643835616438356], [0.0, 0.5, 0.0, 0.0, 0.5, 0.0], [0.5126050420168067, 0.0, 0.0, 0.48739495798319327, 0.0, 0.0]]}\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "print(joint_bayes(discrete_penguin_dataset, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Similarly to Naive Bayes, write the functions used to predict the aposteriori probabilities, the label and the accuracy of the Joint Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex==0 and island==0\n",
      "sex==0 and island==1\n",
      "sex==0 and island==2\n",
      "sex==1 and island==0\n",
      "sex==1 and island==1\n",
      "sex==1 and island==2\n",
      "[1.0, 0.0, 0.0]\n",
      "1\n",
      "0.7027027027027027\n"
     ]
    }
   ],
   "source": [
    "def jb_predict_prob(jb_dict, X, display = False):\n",
    "    # cum ar trebui sa folosesc likelihoods cand el in formatul in care e n-am de unde sa stiu pt ce valori de feature-uri sunt probabilitatile???\n",
    "    # asumam ca ordinea de la product este consistenta...\n",
    "    params = []\n",
    "    for n_feature_class in jb_dict[\"n_features_classes\"]:\n",
    "        params.append(np.unique(n_feature_class))\n",
    "    cartesian_product = list(product(*params))\n",
    "\n",
    "    instance = {}\n",
    "    for column in X.columns:\n",
    "        instance[column] = int(X.iloc[[0]][column])\n",
    "\n",
    "    # caut instance in lista de la product\n",
    "    i_instance = -1\n",
    "    for i in range(len(cartesian_product)):\n",
    "        if cartesian_product[i] == tuple(instance.values()):\n",
    "            i_instance = i\n",
    "    \n",
    "    p_max= -999999999\n",
    "    all_p = []\n",
    "    for class_ in jb_dict[\"classes\"]:\n",
    "        p = 1\n",
    "        for i in range(len(cartesian_product)):\n",
    "            p *= jb_dict[\"likelihoods\"][class_-1][i_instance] * jb_dict[\"prior_probs\"][class_-1]\n",
    "        all_p.append(p)\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(all_p)):\n",
    "        result.append(all_p[i] / sum(all_p))\n",
    "    \n",
    "    if display:\n",
    "        print(instance)\n",
    "        print()\n",
    "        print(cartesian_product)\n",
    "        print()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def jb_predict(jb_dict, X):\n",
    "    prediction = jb_predict_prob(jb_dict, X)\n",
    "    return np.argmax(prediction) + 1\n",
    "\n",
    "def jb_score(jb_dict, X, y):\n",
    "    wrong = 0\n",
    "    for i in range(len(X.index)):\n",
    "        y_label = int(y.iloc[[i]])\n",
    "        y_pred = jb_predict(jb_dict, X.iloc[[i]])\n",
    "        if y_label != y_pred:\n",
    "            wrong += 1\n",
    "    return (len(X.index)-wrong) / len(X.index)\n",
    "\n",
    "\n",
    "jb_dict = joint_bayes(discrete_penguin_dataset, 2)\n",
    "new_instance = pd.DataFrame(\n",
    "  [(1, 2)],\n",
    "columns = [\"sex\",\"island\"])\n",
    "print(jb_predict_prob(jb_dict, new_instance))\n",
    "print(jb_predict(jb_dict, new_instance))\n",
    "X = discrete_penguin_dataset[[\"sex\",\"island\"]]\n",
    "y = discrete_penguin_dataset[\"species\"]\n",
    "print(jb_score(jb_dict, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "17. Calculate the training accuracy of your Joint Bayes algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7027027027027027\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "print(jb_score(jb_dict, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. kNN (2 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will use the entire `penguin_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the Euclidean distance between the test instance `{\"sex\" : 1, \"island\" : 2, \"bill_length\" : 20, \"bill_depth\" : 40, \"flipper_length\" : 355, \"body_mass\" : 855}` and the instances from the dataset. Store the values in an object called `instance_distance`. Print the average distance. (*Hint*: check the norm function from the numpy package.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926.8521996521343\n",
      "3356.130258608873\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "from numpy import linalg as LA\n",
    "test_instance = pd.DataFrame(\n",
    "  [(1, 2, 20, 40, 355, 855)],\n",
    "columns = [\"sex\",\"island\",\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"])\n",
    "\n",
    "test_instance_norm = LA.norm(test_instance.values[0])\n",
    "print(test_instance_norm)\n",
    "\n",
    "instance_distance = []\n",
    "df_no_species = penguin_dataset.drop(columns = [\"species\"])\n",
    "for index, row in df_no_species.iterrows():\n",
    "    line = df_no_species.iloc[[index]]\n",
    "    norm = LA.norm(np.subtract(line.values[0], test_instance.values[0]))\n",
    "    instance_distance.append(norm)\n",
    "\n",
    "print(sum(instance_distance) / len(instance_distance))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the `5` nearest neighbours of the test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{303: 1852.5296677786296, 58: 2002.5142621214961, 52: 2002.7792714126037, 110: 2052.020701649961, 48: 2052.0582009290088, 287: 2052.153941594051, 92: 2052.8267876272466, 98: 2076.5178472625753, 138: 2151.379215759044, 39: 2151.9174263897767, 122: 2201.319243090379, 62: 2201.374654619245, 136: 2201.5134930315553, 118: 2201.680687565751, 96: 2226.8536750312087, 37: 2251.511769900393, 114: 2301.310774319714, 54: 2301.458037853395, 35: 2301.675748232144, 23: 2302.4544230016804, 130: 2325.950689503112, 102: 2326.7007994153437, 106: 2350.7782498568426, 74: 2351.0240322038394, 22: 2351.204299928018, 295: 2351.2197621660125, 7: 2351.574147246903, 2: 2400.525169624347, 321: 2400.8549727128457, 313: 2401.1865691778307, 25: 2401.7239933014785, 271: 2401.772564170055, 120: 2425.726942588551, 281: 2450.36917422661, 289: 2450.474913154591, 27: 2450.874964170959, 88: 2450.9238523462946, 33: 2451.3343815155044, 315: 2475.193642929781, 29: 2475.338150637201, 124: 2475.6727691680094, 113: 2475.754464804618, 13: 2476.0482467027978, 60: 2500.2871215122477, 78: 2500.542767080779, 112: 2500.671207895992, 319: 2500.8541620814276, 297: 2500.859680189994, 329: 2549.7974154822577, 291: 2549.989766646133, 134: 2550.333791879016, 286: 2550.5374923729314, 311: 2550.6166744534544, 86: 2550.812892001293, 132: 2550.8365784581338, 15: 2551.5833770425766, 128: 2575.6463984794186, 41: 2575.988588872241, 142: 2600.0720836161445, 11: 2600.0799391557175, 320: 2600.091782995362, 3: 2600.187758605136, 46: 2600.3783205526074, 42: 2600.3837043790286, 283: 2600.462635763106, 116: 2601.3463898527625, 135: 2625.5609305441762, 141: 2625.7159252287747, 115: 2649.7917521948775, 125: 2649.8974036743384, 308: 2649.9974056591072, 126: 2650.0970718069934, 265: 2650.2425662569076, 81: 2650.344424787088, 44: 2650.5605916484915, 324: 2675.0517228644385, 268: 2675.4231160696804, 84: 2699.4750397067946, 66: 2699.8536997400433, 307: 2699.944373501054, 79: 2699.961070089715, 72: 2700.385648384319, 50: 2700.4550079569926, 99: 2700.5748091841483, 21: 2700.6473890532247, 32: 2700.8528449362066, 279: 2725.2194810693686, 299: 2749.903098292738, 317: 2749.990141800512, 64: 2750.0678627990255, 83: 2750.1013890400477, 56: 2750.4122890941276, 285: 2750.694285085131, 16: 2750.712994843337, 5: 2775.6127341543884, 327: 2799.7884723671536, 267: 2799.943451214685, 4: 2799.9999732142855, 326: 2800.152226219139, 306: 2800.2576113636405, 139: 2800.3109987999546, 314: 2824.615607476529, 309: 2824.7345397399736, 274: 2849.571845032162, 49: 2849.884545029851, 70: 2849.894041889979, 68: 2849.911665297716, 90: 2849.9186953315, 293: 2849.931893923081, 144: 2850.093052866871, 10: 2850.209360731243, 277: 2850.293018270227, 282: 2850.655861376466, 269: 2874.603160437976, 94: 2874.7497995477797, 137: 2874.905996724067, 100: 2899.350065100798, 272: 2899.5596096648883, 53: 2899.603002136672, 143: 2899.6667825803706, 0: 2900.365580405339, 111: 2924.2805166399476, 332: 2924.4513553827496, 276: 2924.673653589405, 330: 2924.72176454445, 323: 2949.202855010147, 292: 2949.2987725898506, 51: 2949.6012950905756, 275: 2949.655979940712, 8: 2949.6815760349455, 288: 2949.6833253757936, 80: 2949.7352321861026, 17: 2949.791594333403, 20: 2949.9037780917533, 76: 2949.9114105342214, 1: 2949.996103387257, 19: 2950.3437562426516, 104: 2974.295355206002, 301: 2999.476395973137, 117: 3024.0254975776907, 300: 3049.205878585439, 36: 3049.348487792105, 266: 3049.365220828755, 108: 3049.5379404099895, 101: 3049.5878803536716, 65: 3049.6074583460736, 129: 3049.6234947940707, 71: 3049.9159332676695, 28: 3049.942625689867, 26: 3050.2674390944803, 305: 3098.5985945262414, 185: 3098.684395029607, 325: 3098.816848411664, 316: 3099.059528308548, 270: 3099.161641476611, 59: 3099.4434274559685, 31: 3099.5169688194965, 82: 3099.572636671062, 18: 3099.7969046374633, 24: 3100.0839440247423, 131: 3123.976800490042, 123: 3148.510093679231, 328: 3148.7490658990278, 145: 3148.915289429044, 119: 3149.0041854529186, 73: 3149.214381397367, 322: 3198.811263579019, 318: 3198.825656393296, 47: 3198.8996936446756, 280: 3198.92003807535, 278: 3198.944608773337, 57: 3199.299837464441, 109: 3224.076950074238, 331: 3248.45219142902, 298: 3248.775375429948, 93: 3249.2455262722146, 61: 3249.430876322806, 162: 3298.3693319578388, 30: 3298.9443538804953, 67: 3298.9545086284534, 290: 3299.00833584882, 273: 3299.052471543913, 43: 3299.208045880102, 160: 3348.337617385678, 191: 3348.3407024972835, 205: 3348.4271725692347, 77: 3349.018120285407, 14: 3349.024522155668, 69: 3398.912780581461, 133: 3398.9921300291358, 97: 3399.113390282825, 140: 3399.1290943416666, 107: 3423.7818403046654, 312: 3448.1492369095627, 187: 3448.243868986067, 89: 3448.262439258358, 201: 3448.3163500467876, 85: 3448.3984427557093, 296: 3448.663694824417, 121: 3448.8489587107174, 45: 3449.169207214978, 183: 3498.2725679969535, 91: 3498.7401075244215, 257: 3522.8972806484153, 207: 3547.992629642852, 168: 3548.0340330949475, 197: 3548.145905962718, 154: 3548.1819922320783, 9: 3548.5553919869985, 38: 3548.7041155892384, 55: 3548.723841044834, 284: 3548.7962578880183, 148: 3598.131251080205, 203: 3598.1698806476606, 310: 3598.493280527282, 63: 3598.5517364628786, 87: 3599.090408700509, 127: 3623.466089257632, 146: 3648.0356974678853, 304: 3648.294568699189, 12: 3648.543372361085, 151: 3698.0344103320617, 294: 3698.2432099579387, 251: 3723.0675376629956, 230: 3747.6660630317638, 173: 3748.0007644076063, 220: 3748.0350705402957, 105: 3748.7290446229904, 40: 3748.7445365081894, 224: 3772.9100254312984, 242: 3773.257203001142, 156: 3797.767567927242, 158: 3797.795668542477, 255: 3797.809933369494, 34: 3798.959969517973, 6: 3823.452078946459, 212: 3847.528023289759, 209: 3847.577975818034, 218: 3847.683973509259, 249: 3847.877984032238, 176: 3847.9254774488554, 75: 3848.4195678226147, 226: 3872.8218291060075, 95: 3873.093079181031, 214: 3897.6585650875063, 188: 3897.6726594212605, 222: 3897.685272055711, 236: 3897.7914900620326, 228: 3897.792253314689, 103: 3923.3083284391505, 152: 3947.791195086184, 302: 3947.840990972154, 164: 3947.8705259924623, 244: 3997.494496556562, 261: 3997.6252613270294, 238: 3997.643098126695, 195: 3997.802796537118, 241: 4022.3701681968555, 232: 4022.7211250595037, 190: 4047.5067844291502, 199: 4047.5215391644306, 240: 4072.5515343577913, 260: 4072.6178963904777, 234: 4097.460097426209, 181: 4097.608503749473, 246: 4122.540974932815, 198: 4147.223289141784, 215: 4147.321824985373, 202: 4147.35219748697, 170: 4147.514559347562, 253: 4147.516761870891, 167: 4147.518446010819, 200: 4197.318806095148, 169: 4197.488216779173, 177: 4197.762887539028, 216: 4247.278728786233, 171: 4247.461738497475, 193: 4247.531742082689, 179: 4297.294014842364, 155: 4297.435677238229, 239: 4347.092146481369, 153: 4347.280641964583, 217: 4347.3452002342765, 263: 4347.499060379427, 243: 4397.095570942256, 175: 4397.166630911319, 182: 4397.230565935791, 206: 4447.167913402865, 196: 4447.216889246577, 231: 4447.296294379316, 194: 4447.341979205107, 227: 4497.073694081519, 165: 4497.128272575734, 184: 4497.560803368866, 235: 4546.946255455413, 192: 4547.025517852302, 180: 4547.155511965695, 150: 4547.311514510525, 264: 4547.379379378853, 225: 4597.133629774101, 256: 4646.8493562843205, 247: 4646.894749184664, 250: 4647.174109068865, 248: 4647.248589219216, 252: 4647.446139548042, 233: 4696.830251350372, 204: 4696.991084726476, 213: 4697.022619702826, 189: 4697.044822013092, 174: 4697.10177556331, 157: 4697.208022857834, 229: 4746.8540034427015, 245: 4746.8912785106, 208: 4796.787777252606, 237: 4796.939905814957, 172: 4797.189577242075, 210: 4846.759874390313, 147: 4846.7635273448195, 186: 4846.894073527912, 149: 4847.093359942637, 166: 4847.099290297239, 262: 4896.961675365655, 219: 4946.721031147805, 211: 4946.750720422448, 258: 4996.745241654811, 161: 4997.051428592666, 159: 4997.163767578565, 254: 5096.702350540004, 223: 5096.827052196298, 259: 5146.854853208899, 221: 5146.919845888412, 178: 5196.705895083924, 163: 5446.783792294312}\n",
      "   sex  island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0  1.0     1.0            46.9           16.6              192.0       2700.0   \n",
      "1  1.0     0.0            36.4           17.1              184.0       2850.0   \n",
      "2  1.0     0.0            36.5           16.6              181.0       2850.0   \n",
      "3  1.0     2.0            38.6           17.0              188.0       2900.0   \n",
      "4  1.0     0.0            34.5           18.1              187.0       2900.0   \n",
      "\n",
      "   species  \n",
      "0      2.0  \n",
      "1      1.0  \n",
      "2      1.0  \n",
      "3      1.0  \n",
      "4      1.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.2, 0. ]])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution here\n",
    "line_index_dict = {}\n",
    "for i in range(len(df_no_species.index)):\n",
    "    line_index_dict[i] = LA.norm(np.subtract(df_no_species.iloc[[i]].values[0], test_instance.values[0]))\n",
    "\n",
    "line_index_dict = {k: v for k, v in sorted(line_index_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "print(line_index_dict)\n",
    "\n",
    "i = 0\n",
    "result_dataframe = pd.DataFrame(columns = [\"sex\",\"island\",\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\",\"species\"])\n",
    "for key,value in line_index_dict.items():\n",
    "    if i == 5:\n",
    "        break\n",
    "    result_dataframe.loc[i] = penguin_dataset.iloc[[key]].values[0]\n",
    "    i += 1\n",
    "\n",
    "print(result_dataframe)\n",
    "\n",
    "# verificare daca nu cumva ceea ce am facut da identic cu k-nn... raspunsul este ca da. lol\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "X, y = penguin_dataset[['sex', 'island',\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"]], penguin_dataset[\"species\"]\n",
    "knn.fit(X, y)\n",
    "knn.predict(test_instance)\n",
    "knn.predict_proba(test_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine the probabilities of the labels that kNN would assign for this test instance (`k = 5`). Which label has the highest probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8, 0.2, 0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "species_vals, nr_species_vals = np.unique(result_dataframe[\"species\"], return_counts=True)\n",
    "all_species_vals = np.unique(penguin_dataset[\"species\"])\n",
    "probas = [0 for i in range(len(all_species_vals))]\n",
    "for i in range(len(all_species_vals)):\n",
    "    if all_species_vals[i] not in species_vals:\n",
    "        probas[i] = 0\n",
    "    else:\n",
    "        probas[i] = (nr_species_vals[i] / sum(nr_species_vals))\n",
    "print(probas)\n",
    "print(np.argmax(probas)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suppose that kNN gives for each neighbour a weight that is equal to the inverse of the distance. Print the changed probabilities, as well as the label predicted by kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2.0: 0.0005398024211936649}, {1.0: 0.0004993722236667537}, {1.0: 0.0004993061463506552}, {1.0: 0.0004873245183130626}, {1.0: 0.00048731561295253686}]\n",
      "{2.0: 0.0005398024211936649, 1.0: 0.001973318501283008}\n",
      "[0.7852063478657878, 0.2147936521342122, 0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "def get_value_from_dict_by_index(dict_, index):\n",
    "    i = 0\n",
    "    for key,value in dict_.items():\n",
    "        if i == index:\n",
    "            return value\n",
    "        i += 1\n",
    "\n",
    "#lista cu ponderile noi\n",
    "species_weight_list = []\n",
    "for i in range(len(result_dataframe.index)):\n",
    "    species_weight_dict = {}\n",
    "    species_weight_dict[result_dataframe.iloc[[i]][\"species\"].values[0]] = 1 / get_value_from_dict_by_index(line_index_dict, i)\n",
    "    species_weight_list.append(species_weight_dict)\n",
    "\n",
    "print(species_weight_list)\n",
    "\n",
    "species_weight_dict_all = {}\n",
    "for weight in species_weight_list:\n",
    "    if list(weight.keys())[0] not in species_weight_dict_all.keys():\n",
    "        species_weight_dict_all[list(weight.keys())[0]] = weight[list(weight.keys())[0]]\n",
    "    else:\n",
    "        species_weight_dict_all[list(weight.keys())[0]] += weight[list(weight.keys())[0]]\n",
    "\n",
    "print(species_weight_dict_all)\n",
    "\n",
    "changed_probas = []\n",
    "for i in range(len(all_species_vals)):\n",
    "    if all_species_vals[i] not in species_weight_dict_all.keys():\n",
    "        changed_probas.append(0)\n",
    "    else:\n",
    "        changed_probas.append(species_weight_dict_all[all_species_vals[i]] / sum(species_weight_dict_all.values()))\n",
    "\n",
    "print(changed_probas)\n",
    "print(np.argmax(changed_probas)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **k-NN implementation**: Create a function `knn_predict_prob` that will take six arguments:\n",
    "- `df`: the dataframe containing the features and the target feature\n",
    "- `test_x`: a list with the attributes observed for *one* instance\n",
    "- `k`: the number of nearest neighbours\n",
    "- `use_weights`: boolean value that indicates whether to assign weights based on the inverse of the distance or not\n",
    "- `p`: either an integer, indicating the order of the Minkowski distance (p=2 is the equivalent for Euclidean) or a custom distance function\n",
    "- `index_target`: the index of the column that contains the labels of the target feature\n",
    "\n",
    "The function should:\n",
    "- calculate the distance between `test_x` and the observations from the dataset\n",
    "- extract the k-nearest neighbours\n",
    "- calculate the weight for each label\n",
    "- normalize the weights to become probabilities\n",
    "- return the probability vector, that will indicate the probability of the instance `test_x` to have the label `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8, 0.2, 0]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def knn_predict_prob(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "\n",
    "    def get_value_from_dict_by_index(dict_, index):\n",
    "        i = 0\n",
    "        for key,value in dict_.items():\n",
    "            if i == index:\n",
    "                return value\n",
    "            i += 1\n",
    "\n",
    "    df_no_target = df.drop(columns = [df.columns[index_target]])\n",
    "    line_index_dict = {}\n",
    "    for i in range(len(df_no_target.index)):\n",
    "        line = df_no_target.iloc[[i]]\n",
    "        norm = LA.norm(np.subtract(line.values[0], test_x.values[0]),ord=p)\n",
    "        line_index_dict[i] = norm\n",
    "\n",
    "    line_index_dict_copy = copy.deepcopy(line_index_dict)\n",
    "\n",
    "    line_index_dict = {k: v for k, v in sorted(line_index_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "    line_index_sorted_old_distance = {}\n",
    "    for key,value in line_index_dict.items():\n",
    "        line_index_sorted_old_distance[key] = line_index_dict_copy[key]\n",
    "    \n",
    "    # print(line_index_dict_copy)\n",
    "    # print(line_index_old_distance)\n",
    "\n",
    "    i = 0\n",
    "    result_dataframe = pd.DataFrame(columns = df.columns)\n",
    "    for key,value in line_index_dict.items():\n",
    "        if i == k:\n",
    "            break\n",
    "        result_dataframe.loc[i] = df.iloc[[key]].values[0]\n",
    "        i += 1\n",
    "    \n",
    "    target_feature = df.columns[index_target]\n",
    "    if not use_weights:\n",
    "        species_vals, nr_species_vals = np.unique(result_dataframe[target_feature], return_counts=True)\n",
    "        all_species_vals = np.unique(df[target_feature])\n",
    "        probas = [0 for i in range(len(all_species_vals))]\n",
    "        for i in range(len(all_species_vals)):\n",
    "            if all_species_vals[i] not in species_vals:\n",
    "                probas[i] = 0\n",
    "            else:\n",
    "                probas[all_species_vals[i]-1] = (nr_species_vals[list(species_vals).index(all_species_vals[i])] / sum(nr_species_vals))\n",
    "        return probas\n",
    "    else:\n",
    "        species_weight_list = []\n",
    "        for i in range(len(result_dataframe.index)):\n",
    "            species_weight_dict = {}\n",
    "            val = 0\n",
    "            if get_value_from_dict_by_index(line_index_sorted_old_distance, i) == 0:\n",
    "                val  = 1e-10\n",
    "            else:\n",
    "                val = get_value_from_dict_by_index(line_index_sorted_old_distance, i)\n",
    "            species_weight_dict[result_dataframe.iloc[[i]][\"species\"].values[0]] = 1 / val\n",
    "            species_weight_list.append(species_weight_dict)\n",
    "\n",
    "        #print(species_weight_list)\n",
    "        \n",
    "        species_weight_dict_all = {}\n",
    "        for weight in species_weight_list:\n",
    "            if list(weight.keys())[0] not in species_weight_dict_all.keys():\n",
    "                species_weight_dict_all[list(weight.keys())[0]] = weight[list(weight.keys())[0]]\n",
    "            else:\n",
    "                species_weight_dict_all[list(weight.keys())[0]] += weight[list(weight.keys())[0]]\n",
    "\n",
    "        all_species_vals = np.unique(df[target_feature])\n",
    "        changed_probas = [0 for i in range(len(all_species_vals))]\n",
    "        for i in range(len(all_species_vals)):\n",
    "            if all_species_vals[i] not in species_weight_dict_all.keys():\n",
    "                changed_probas[i] = 0\n",
    "            else:\n",
    "                changed_probas[all_species_vals[i]-1] = species_weight_dict_all[all_species_vals[i]] / sum(species_weight_dict_all.values())\n",
    "\n",
    "        return changed_probas\n",
    "        \n",
    "print(knn_predict_prob(penguin_dataset, test_instance, k=5, use_weights = False, p = 2, index_target = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function that, based on the probabilities calculated above, returns the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def knn_predict(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "    p = knn_predict_prob(df, test_x, k, use_weights, p, index_target)\n",
    "    return np.argmax(p)+1\n",
    "\n",
    "print(knn_predict(penguin_dataset, test_instance, 5, use_weights = False, p = 2, index_target = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculate the probabilities and the predicted label for the instance from exercise 1 using the following configurations:\n",
    "- `k = 11, unweighted, Euclidean distance`\n",
    "- `k = 11, weighted, Euclidean distance`\n",
    "- `k = 11, unweighted, Manhattan distance`\n",
    "- `k = 11, weighted, Manhattan distance`\n",
    "\n",
    "Compare your results with the results obtained by the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritm personal\n",
      "\n",
      "K=11 unweighted euclidean\n",
      "Probabilities:  [0.8181818181818182, 0.18181818181818182, 0]\n",
      "Prediction:  1\n",
      "\n",
      "K=11 weighted euclidean\n",
      "Probabilities:  [0.8081241299001586, 0.19187587009984133, 0]\n",
      "Prediction:  1\n",
      "\n",
      "K=11 unweighted manhattan\n",
      "Probabilities:  [0.8181818181818182, 0.18181818181818182, 0]\n",
      "Prediction:  1\n",
      "\n",
      "K=11 weighted manhattan\n",
      "Probabilities:  [0.8094697299455473, 0.19053027005445272, 0]\n",
      "Prediction:  1\n",
      "\n",
      "Algoritm sklearn\n",
      "\n",
      "K=11 unweighted euclidean\n",
      "[[0.81818182 0.18181818 0.        ]]\n",
      "[1]\n",
      "K=11 weighted euclidean\n",
      "[[0.80812413 0.19187587 0.        ]]\n",
      "[1]\n",
      "K=11 unweighted manhattan\n",
      "[[0.81818182 0.18181818 0.        ]]\n",
      "[1]\n",
      "K=11 weighted manhattan\n",
      "[[0.80946973 0.19053027 0.        ]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "\n",
    "print(\"Algoritm personal\\n\")\n",
    "\n",
    "print(\"K=11 unweighted euclidean\")\n",
    "print(\"Probabilities: \",knn_predict_prob(penguin_dataset, test_instance, 11, use_weights = False, p = 2, index_target = 6))\n",
    "print(\"Prediction: \", knn_predict(penguin_dataset, test_instance, 11, use_weights = False, p = 2, index_target = 6))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"K=11 weighted euclidean\")\n",
    "print(\"Probabilities: \",knn_predict_prob(penguin_dataset, test_instance, 11, use_weights = True, p = 2, index_target = 6))\n",
    "print(\"Prediction: \", knn_predict(penguin_dataset, test_instance, 11, use_weights = True, p = 2, index_target = 6))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"K=11 unweighted manhattan\")\n",
    "print(\"Probabilities: \",knn_predict_prob(penguin_dataset, test_instance, 11, use_weights = False, p = 1, index_target = 6))\n",
    "print(\"Prediction: \", knn_predict(penguin_dataset, test_instance, 11, use_weights = False, p = 1, index_target = 6))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"K=11 weighted manhattan\")\n",
    "print(\"Probabilities: \",knn_predict_prob(penguin_dataset, test_instance, 11, use_weights = True, p = 1, index_target = 6))\n",
    "print(\"Prediction: \", knn_predict(penguin_dataset, test_instance, 11, use_weights = True, p = 1, index_target = 6))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Algoritm sklearn\\n\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, y = penguin_dataset[['sex', 'island',\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"]], penguin_dataset[\"species\"]\n",
    "\n",
    "print(\"K=11 unweighted euclidean\")\n",
    "knn = KNeighborsClassifier(n_neighbors=11, weights='uniform', p=2)\n",
    "knn.fit(X, y)\n",
    "print(knn.predict_proba(test_instance))\n",
    "print(knn.predict(test_instance))\n",
    "\n",
    "print(\"K=11 weighted euclidean\")\n",
    "knn = KNeighborsClassifier(n_neighbors=11, weights='distance', p=2)\n",
    "knn.fit(X, y)\n",
    "print(knn.predict_proba(test_instance))\n",
    "print(knn.predict(test_instance))\n",
    "\n",
    "print(\"K=11 unweighted manhattan\")\n",
    "knn = KNeighborsClassifier(n_neighbors=11, weights='uniform', p=1)\n",
    "knn.fit(X, y)\n",
    "print(knn.predict_proba(test_instance))\n",
    "print(knn.predict(test_instance))\n",
    "\n",
    "print(\"K=11 weighted manhattan\")\n",
    "knn = KNeighborsClassifier(n_neighbors=11, weights='distance', p=1)\n",
    "knn.fit(X, y)\n",
    "print(knn.predict_proba(test_instance))\n",
    "print(knn.predict(test_instance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function that calculates the accuracy of the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test y pentru ce?\n",
    "def knn_score(df, test_x, test_y, k, use_weights = True, p = 2, index_target = -1):\n",
    "    wrong = 0\n",
    "    for i in range(len(test_x.index)):\n",
    "        if knn_predict(df, test_x.iloc[[i]], k, use_weights, p, index_target) != test_y.iloc[[i]].values[0]:\n",
    "            wrong += 1\n",
    "    return (len(test_x.index) - wrong) / len(test_x.index)\n",
    "\n",
    "test_x = penguin_dataset[['sex', 'island',\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"]]\n",
    "test_y = penguin_dataset[\"species\"]\n",
    "print(knn_score(penguin_dataset, test_x, test_y, 11, use_weights = True, p = 2, index_target = 6))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=11, weights='distance', p=2)\n",
    "knn.fit(test_x, test_y)\n",
    "print(knn.score(test_x, test_y))\n",
    "\n",
    "\n",
    "\n",
    "# troubleshooting acuratete... a inceput cu 0.43 si a ajuns mai buna decat cea de la sklearn\n",
    "# df_no_species = penguin_dataset.drop(columns=['species'])\n",
    "# line = df_no_species.iloc[[160]]\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=5, weights='distance', p=2)\n",
    "# knn.fit(X, y)\n",
    "\n",
    "# print(knn_predict_prob(penguin_dataset, line, 5, use_weights = True, p = 2, index_target = 6))\n",
    "# print(knn.predict_proba(line))\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', p=2)\n",
    "# knn.fit(X, y)\n",
    "\n",
    "# print(knn.predict_proba(line))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the training accuracy of the unweighted kNN when k varies from 3 to 15? (use only odd numbers). Does adding the weight / changing the distance metric improve the score? Justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-nn fara weight euclidian 0.9159159159159159\n",
      "3-nn fara weight euclidian sklearn 0.9159159159159159\n",
      "3-nn fara weight manhattan 0.924924924924925\n",
      "3-nn fara weight manhattan sklearn 0.924924924924925\n",
      "3-nn cu weight euclidian 1.0\n",
      "3-nn cu weight euclidian sklearn 1.0\n",
      "3-nn cu weight manhattan 1.0\n",
      "3-nn cu weight manhattan sklearn 1.0\n",
      "5-nn fara weight euclidian 0.8378378378378378\n",
      "5-nn fara weight euclidian sklearn 0.8378378378378378\n",
      "5-nn fara weight manhattan 0.8378378378378378\n",
      "5-nn fara weight manhattan sklearn 0.8378378378378378\n",
      "5-nn cu weight euclidian 1.0\n",
      "5-nn cu weight euclidian sklearn 1.0\n",
      "5-nn cu weight manhattan 1.0\n",
      "5-nn cu weight manhattan sklearn 1.0\n",
      "7-nn fara weight euclidian 0.8438438438438438\n",
      "7-nn fara weight euclidian sklearn 0.8438438438438438\n",
      "7-nn fara weight manhattan 0.8498498498498499\n",
      "7-nn fara weight manhattan sklearn 0.8498498498498499\n",
      "7-nn cu weight euclidian 1.0\n",
      "7-nn cu weight euclidian sklearn 1.0\n",
      "7-nn cu weight manhattan 1.0\n",
      "7-nn cu weight manhattan sklearn 1.0\n",
      "9-nn fara weight euclidian 0.8138138138138138\n",
      "9-nn fara weight euclidian sklearn 0.8138138138138138\n",
      "9-nn fara weight manhattan 0.8228228228228228\n",
      "9-nn fara weight manhattan sklearn 0.8228228228228228\n",
      "9-nn cu weight euclidian 1.0\n",
      "9-nn cu weight euclidian sklearn 1.0\n",
      "9-nn cu weight manhattan 1.0\n",
      "9-nn cu weight manhattan sklearn 1.0\n",
      "11-nn fara weight euclidian 0.8018018018018018\n",
      "11-nn fara weight euclidian sklearn 0.8018018018018018\n",
      "11-nn fara weight manhattan 0.8078078078078078\n",
      "11-nn fara weight manhattan sklearn 0.8078078078078078\n",
      "11-nn cu weight euclidian 1.0\n",
      "11-nn cu weight euclidian sklearn 1.0\n",
      "11-nn cu weight manhattan 1.0\n",
      "11-nn cu weight manhattan sklearn 1.0\n",
      "13-nn fara weight euclidian 0.8198198198198198\n",
      "13-nn fara weight euclidian sklearn 0.8198198198198198\n",
      "13-nn fara weight manhattan 0.8228228228228228\n",
      "13-nn fara weight manhattan sklearn 0.8228228228228228\n",
      "13-nn cu weight euclidian 1.0\n",
      "13-nn cu weight euclidian sklearn 1.0\n",
      "13-nn cu weight manhattan 1.0\n",
      "13-nn cu weight manhattan sklearn 1.0\n",
      "15-nn fara weight euclidian 0.7897897897897898\n",
      "15-nn fara weight euclidian sklearn 0.7897897897897898\n",
      "15-nn fara weight manhattan 0.7897897897897898\n",
      "15-nn fara weight manhattan sklearn 0.7897897897897898\n",
      "15-nn cu weight euclidian 1.0\n",
      "15-nn cu weight euclidian sklearn 1.0\n",
      "15-nn cu weight manhattan 1.0\n",
      "15-nn cu weight manhattan sklearn 1.0\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "for k in range(3,16,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', p=2)\n",
    "    knn.fit(test_x, test_y)\n",
    "    print(str(k) + \"-nn fara weight euclidian\",knn_score(penguin_dataset, test_x, test_y, k, use_weights = False, p = 2, index_target = 6))\n",
    "    print(str(k) + \"-nn fara weight euclidian sklearn\",knn.score(test_x, test_y))\n",
    "    print(str(k) + \"-nn fara weight manhattan\",knn_score(penguin_dataset, test_x, test_y, k, use_weights = False, p = 1, index_target = 6))\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', p=1)\n",
    "    knn.fit(test_x, test_y)\n",
    "    print(str(k) + \"-nn fara weight manhattan sklearn\",knn.score(test_x, test_y))\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance', p=2)\n",
    "    knn.fit(test_x, test_y)\n",
    "    print(str(k) + \"-nn cu weight euclidian\",knn_score(penguin_dataset, test_x, test_y, k, use_weights = True, p = 2, index_target = 6))\n",
    "    print(str(k) + \"-nn cu weight euclidian sklearn\",knn.score(test_x, test_y))\n",
    "    print(str(k) + \"-nn cu weight manhattan\",knn_score(penguin_dataset, test_x, test_y, k, use_weights = True, p = 1, index_target = 6))\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance', p=1)\n",
    "    knn.fit(test_x, test_y)\n",
    "    print(str(k) + \"-nn cu weight manhattan sklearn\",knn.score(test_x, test_y))\n",
    "\n",
    "# acuratetea creste cand se face knn cu pondere pentru ca punctele mai apropiate in plan capata prioritate.\n",
    "# exista o diferenta intre distanta manhattan si euclidiana, dar nu este foarte mare. diferenta este modul de calcul a distantei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. AdaBoost (3 points; 0.25 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Testing the performance (1.5 points; 0.1 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise grading\n",
    "\n",
    "| Section | Exercise | Points |\n",
    "| --- | --- | --- |\n",
    "| I |\t1 | \t0.1 |\n",
    "| I |\t2 | \t0.1 |\n",
    "| I |\t3 | \t0.25 |\n",
    "| I |\t4 | \t0.25 |\n",
    "| I |\t5 | \t0.15 |\n",
    "| I |\t6 | \t0.65 |\n",
    "| I |\t7 | \t0.1 |\n",
    "| I |\t8 | \t0.25 |\n",
    "| I |\t9 | \t0.15 |\n",
    "| I |\t10 | \t0.1 |\n",
    "| I |\t11 | \t0.1 |\n",
    "| I |\t12 | \t0.1 |\n",
    "| I |\t13 | \t0.15 |\n",
    "| I |\t14 | \t0.65 |\n",
    "| I |\t15 | \t0.1 |\n",
    "| I |\t16 | \t0.2 |\n",
    "| I |\t17 | \t0.1 |\n",
    "|II | \t1 | \t0.2 |\n",
    "|II | \t2 | \t0.1 |\n",
    "|II | \t3 | \t0.15 |\n",
    "|II | \t4 | \t0.2 |\n",
    "|II | \t5 | \t0.72 |\n",
    "|II | \t6 | \t0.1 |\n",
    "|II | \t7 | \t0.28 |\n",
    "|II | \t8 | \t0.1 |\n",
    "|II | \t9 | \t0.15 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
