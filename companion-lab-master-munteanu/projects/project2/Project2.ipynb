{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Machine Learning, Fall 2022\n",
    "\n",
    "Deadline: 11th of December 2022, 23:59\n",
    "\n",
    "To do this project you have to complete this Jupyter notebook and send it via Discord.\n",
    "\n",
    "The total number of points allocated for this project is 10.\n",
    "\n",
    "You will need the following modules to solve the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sex  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0      0       2            39.1           18.7              181.0   \n",
      "1      1       2            39.5           17.4              186.0   \n",
      "2      1       2            40.3           18.0              195.0   \n",
      "3      1       2            36.7           19.3              193.0   \n",
      "4      0       2            39.3           20.6              190.0   \n",
      "..   ...     ...             ...            ...                ...   \n",
      "328    0       1            55.8           19.8              207.0   \n",
      "329    1       1            43.5           18.1              202.0   \n",
      "330    0       1            49.6           18.2              193.0   \n",
      "331    0       1            50.8           19.0              210.0   \n",
      "332    1       1            50.2           18.7              198.0   \n",
      "\n",
      "     body_mass_g  species  \n",
      "0         3750.0        1  \n",
      "1         3800.0        1  \n",
      "2         3250.0        1  \n",
      "3         3450.0        1  \n",
      "4         3650.0        1  \n",
      "..           ...      ...  \n",
      "328       4000.0        2  \n",
      "329       3400.0        2  \n",
      "330       3775.0        2  \n",
      "331       4100.0        2  \n",
      "332       3775.0        2  \n",
      "\n",
      "[333 rows x 7 columns]\n",
      "   sex  island  species\n",
      "0    0       2        1\n",
      "1    1       2        1\n",
      "2    1       2        1\n",
      "3    1       2        1\n",
      "4    0       2        1\n"
     ]
    }
   ],
   "source": [
    "penguin_dataset = pd.read_csv(\"data/penguins_filtered.csv\")\n",
    "\n",
    "penguin_dataset = penguin_dataset.replace({\n",
    "    \"Adelie\": 1,\n",
    "    \"Chinstrap\" : 2,\n",
    "    \"Gentoo\": 3,\n",
    "    \"male\" : 0,\n",
    "    \"female\" : 1,\n",
    "    \"Biscoe\" : 0,\n",
    "    \"Dream\" : 1,\n",
    "    \"Torgersen\" : 2})\n",
    "\n",
    "discrete_penguin_dataset = penguin_dataset[[\"sex\", \"island\", \"species\"]]\n",
    "print(penguin_dataset)\n",
    "print(discrete_penguin_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Naive and Joint Bayes (3.5 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the prior probabilities for the target feature. Transform them by applying the natural logarithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43843843843843844, 0.2042042042042042, 0.35735735735735735]\n",
      "[-0.3580913777218827, -0.6899353208000835, -0.4468972721137891]\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "species, species_nr = np.unique(penguin_dataset[\"species\"], return_counts=True)\n",
    "p_species = []\n",
    "for specie_nr in species_nr:\n",
    "    p_species.append(specie_nr / sum(species_nr))\n",
    "\n",
    "print(p_species)\n",
    "\n",
    "for i in range(len(p_species)):\n",
    "    p_species[i] = math.log(p_species[i], 10)\n",
    "\n",
    "print(p_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the formulas used to calculate the maximum aposteriori probability using Naive Bayes and Joint Bayes. Use the names of the variables from the discrete penguin dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer here*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAIVE BAYES: v_MAP = argmax P(a_i|v_j)P(v_j), where a_i is an attribute and v_j is target atrribute value\n",
    "#JOINT BAYES: v_MAP = argmax P(a_1,a_2,...,a_n|v_j)P(v_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find and calculate the logarithm of all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sex=1|species=1) = 0.5\n",
      "ln(P(sex=1|species=1) = 0.5) = -0.30102999566398114\n",
      "\n",
      "P(sex=1|species=2) = 0.5\n",
      "ln(P(sex=1|species=2) = 0.5) = -0.30102999566398114\n",
      "\n",
      "P(sex=1|species=3) = 0.48739495798319327\n",
      "ln(P(sex=1|species=3) = 0.48739495798319327) = -0.3121189678295935\n",
      "\n",
      "P(island=2|species=1) = 0.3219178082191781\n",
      "ln(P(island=2|species=1) = 0.3219178082191781) = -0.4922549978487196\n",
      "\n",
      "P(island=2|species=2) = 1.0\n",
      "ln(P(island=2|species=2) = 1.0) = 0.0\n",
      "\n",
      "P(island=2|species=3) = 1.0\n",
      "ln(P(island=2|species=3) = 1.0) = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "#P(sex=1|species=1),P(island=2|species=1)\n",
    "#P(sex=1|species=2),P(island=2|species=2)\n",
    "#P(sex=1|species=3),P(island=2|species=3)\n",
    "\n",
    "def calculate_conditional_probability(dataset, feature, target_feature, alpha):\n",
    "    feature_vals, nr_feature_vals = np.unique(dataset[feature], return_counts=True)\n",
    "    tf_name = list(target_feature.keys())[0]\n",
    "    tf_val = target_feature[tf_name]\n",
    "    for feature_val in feature_vals:\n",
    "        temp_dataset = dataset[dataset[feature] == feature_val]\n",
    "        target_feature_vals, nr_target_feature_vals = np.unique(temp_dataset[tf_name], return_counts=True)\n",
    "        i_tf_val = 0\n",
    "        #print(target_feature_vals, nr_target_feature_vals)\n",
    "        for i in range(len(target_feature_vals)):\n",
    "            if target_feature_vals[i] == tf_val:\n",
    "                i_tf_val = i\n",
    "                break\n",
    "        if alpha==0:\n",
    "            p_tf_val = nr_target_feature_vals[i_tf_val] / sum(nr_target_feature_vals)\n",
    "        else:\n",
    "            p_tf_val = (nr_target_feature_vals[i_tf_val] + alpha) / (sum(nr_target_feature_vals) + len(target_feature_vals))\n",
    "        res_string = \"P(\" + tf_name + \"=\" + str(tf_val) + \"|\" + feature + \"=\" + str(feature_val) + \") = \" + str(p_tf_val)\n",
    "        print(res_string)\n",
    "        print(\"ln(\" + res_string + \") = \" + str(math.log(p_tf_val, 10)))\n",
    "        print()\n",
    "\n",
    "\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"sex\" : 1},0)\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"island\" : 2},0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why does the results contain infinity? Fix the calculation by using the Laplace Smoothing with `alpha = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sex=1|species=1) = 0.5\n",
      "ln(P(sex=1|species=1) = 0.5) = -0.30102999566398114\n",
      "\n",
      "P(sex=1|species=2) = 0.5\n",
      "ln(P(sex=1|species=2) = 0.5) = -0.30102999566398114\n",
      "\n",
      "P(sex=1|species=3) = 0.48760330578512395\n",
      "ln(P(sex=1|species=3) = 0.48760330578512395) = -0.3119333586743059\n",
      "\n",
      "P(island=2|species=1) = 0.3221476510067114\n",
      "ln(P(island=2|species=1) = 0.3221476510067114) = -0.4919450310366868\n",
      "\n",
      "P(island=2|species=2) = 1.0\n",
      "ln(P(island=2|species=2) = 1.0) = 0.0\n",
      "\n",
      "P(island=2|species=3) = 1.0\n",
      "ln(P(island=2|species=3) = 1.0) = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"sex\" : 1},1)\n",
    "calculate_conditional_probability(penguin_dataset,\"species\",{ \"island\" : 2},1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the aposteriori probabilities of the labels and decide which label will Naive Bayes predict for the instance. Use only the logarithm values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here\n",
    "#P(sex=1|species=1)P(island=2|species=1)P(species=1)\n",
    "#P(sex=1|species=2)P(island=2|species=2)P(species=2)\n",
    "#P(sex=1|species=3)P(island=2|species=3)P(species=3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Naive Bayes implemenation**: write a function called `naive_bayes` that takes three arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "- `alpha`: the parameter used for Laplace Smoothing\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `log_prior`: the logarithmic values of the prior probabilities (the probability of the labels)\n",
    "- `log_likelihoods`: a n x m x t array, where n - the number of features; m - the number of labels; t - the number of values for a feature; this array will contain the logarithmic values of the likelihoods (P(feature = value | target_feature = label))\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(df, index_target = -1, alpha = 1e-10):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the discrete penguin dataset using your version of Naive Bayes and sklearn's. Compare the values of the parameters.(Be careful at what type of Naive Bayes classifier you pick from sklearn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a function named `nb_predict_prob` that uses the log probabilities calculated by Naive Bayes to infer the aposteriori probability of a new instance `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predict_prob(nb_dict, X, use_log = False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a function that does the Naive Bayes prediction using the Maximum Aposteriori Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_predict(nb_dict, X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a function that calculate the accuracy of the trained model on a set of instances `X` with known labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_score(nb_dict, X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Calculate the training accuracy of your Naive Bayes algorithm. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Find and calculate all the conditional probabilities (also names likelihoods) used to predict the label for the instance `{\"sex\" : 1, \"island\" : 2}` in Joint Bayes. (*Hint*: panda's query function might provide itself useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Calculate the aposteriori probabilities of the labels and decide which label will Joint Bayes predict for the instance. Use the conditional and prior probabilities (*not* the logarithmic values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **Joint Bayes implemenation**: write a function called `joint_bayes` that takes two arguments:\n",
    "- `df`: the dataframe which will be used for training\n",
    "- `index_target`: the index of the column associated with the target feature\n",
    "\n",
    "The function should return a dictionary with the following fields:\n",
    "- `prior_probs`: the prior probabilities (the probability of the labels)\n",
    "- `likelihoods`: a n x m array, where n - the number of labels; m - the number of combination between the values of the features; each label will have assigned a list containing the joint probability P(feature_1 = value_1, feature_2 = value_2, ...,  feature_n = value_n | target_feature = label)\n",
    "- `n_classes`: the number of labels\n",
    "- `n_feature_classes`: a vector that contains the number of unique values for each attribute\n",
    "- `classes`: the name of the labels (the values of the target feature).\n",
    "\n",
    "*Hint*: check the imports from the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_bayes(df, index_target = -1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Train the Joint Bayes algorithm on the discrete penguin dataset. Print the obtained dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Similarly to Naive Bayes, write the functions used to predict the aposteriori probabilities, the label and the accuracy of the Joint Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb_predict_prob(jb_dict, X):\n",
    "    pass\n",
    "\n",
    "def jb_predict(jb_dict, X):\n",
    "    pass\n",
    "\n",
    "def jb_score(jb_dict, X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "17. Calculate the training accuracy of your Joint Bayes algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. kNN (2 points; 0.15 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will use the entire `penguin_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the Euclidean distance between the test instance `{\"sex\" : 1, \"island\" : 2, \"bill_length\" : 20, \"bill_depth\" : 40, \"flipper_length\" : 355, \"body_mass\" : 855}` and the instances from the dataset. Store the values in an object called `instance_distance`. Print the average distance. (*Hint*: check the norm function from the numpy package.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the `5` nearest neighbours of the test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine the probabilities of the labels that kNN would assign for this test instance (`k = 5`). Which label has the highest probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suppose that kNN gives for each neighbour a weight that is equal to the inverse of the distance. Print the changed probabilities, as well as the label predicted by kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **k-NN implementation**: Create a function `knn_predict_prob` that will take six arguments:\n",
    "- `df`: the dataframe containing the features and the target feature\n",
    "- `test_x`: a list with the attributes observed for *one* instance\n",
    "- `k`: the number of nearest neighbours\n",
    "- `use_weights`: boolean value that indicates whether to assign weights based on the inverse of the distance or not\n",
    "- `p`: either an integer, indicating the order of the Minkowski distance (p=2 is the equivalent for Euclidean) or a custom distance function\n",
    "- `index_target`: the index of the column that contains the labels of the target feature\n",
    "\n",
    "The function should:\n",
    "- calculate the distance between `test_x` and the observations from the dataset\n",
    "- extract the k-nearest neighbours\n",
    "- calculate the weight for each label\n",
    "- normalize the weights to become probabilities\n",
    "- return the probability vector, that will indicate the probability of the instance `test_x` to have the label `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict_prob(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "   pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a function that, based on the probabilities calculated above, returns the label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(df, test_x, k, use_weights = True, p = 2, index_target = -1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculate the probabilities and the predicted label for the instance from exercise 1 using the following configurations:\n",
    "- `k = 11, unweighted, Euclidean distance`\n",
    "- `k = 11, weighted, Euclidean distance`\n",
    "- `k = 11, unweighted, Manhattan distance`\n",
    "- `k = 11, weighted, Manhattan distance`\n",
    "\n",
    "Compare your results with the results obtained by the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a function that calculates the accuracy of the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_score(df, test_x, test_y, k, use_weights = True, p = 2, index_target = -1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the training accuracy of the unweighted kNN when k varies from 3 to 15? (use only odd numbers). Does adding the weight / changing the distance metric improve the score? Justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. AdaBoost (3 points; 0.25 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Testing the performance (1.5 points; 0.1 bonus per week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise grading\n",
    "\n",
    "| Section | Exercise | Points |\n",
    "| --- | --- | --- |\n",
    "| I |\t1 | \t0.1 |\n",
    "| I |\t2 | \t0.1 |\n",
    "| I |\t3 | \t0.25 |\n",
    "| I |\t4 | \t0.25 |\n",
    "| I |\t5 | \t0.15 |\n",
    "| I |\t6 | \t0.65 |\n",
    "| I |\t7 | \t0.1 |\n",
    "| I |\t8 | \t0.25 |\n",
    "| I |\t9 | \t0.15 |\n",
    "| I |\t10 | \t0.1 |\n",
    "| I |\t11 | \t0.1 |\n",
    "| I |\t12 | \t0.1 |\n",
    "| I |\t13 | \t0.15 |\n",
    "| I |\t14 | \t0.65 |\n",
    "| I |\t15 | \t0.1 |\n",
    "| I |\t16 | \t0.2 |\n",
    "| I |\t17 | \t0.1 |\n",
    "|II | \t1 | \t0.2 |\n",
    "|II | \t2 | \t0.1 |\n",
    "|II | \t3 | \t0.15 |\n",
    "|II | \t4 | \t0.2 |\n",
    "|II | \t5 | \t0.72 |\n",
    "|II | \t6 | \t0.1 |\n",
    "|II | \t7 | \t0.28 |\n",
    "|II | \t8 | \t0.1 |\n",
    "|II | \t9 | \t0.15 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
